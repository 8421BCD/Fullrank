## Sliding Windows Are Not the End: Exploring Full Ranking with Long-Context Large Language Models

## üì¢ Latest News

- Dec, 2024: We have released the [dataset](https://huggingface.co/datasets/liuwenhan/msmarco_full_ranking_list), trained model [$\text{RankMistral}_{100}$](https://huggingface.co/liuwenhan/RankMistral100) and codes. 

## üìã Introduction

This repository contains the code for our paper [Sliding Windows Are Not the End: Exploring Full Ranking with Long-Context Large Language Models](). 

<img src="https://8421bcd.oss-cn-beijing.aliyuncs.com/img/image-20241218172223836.png" alt="image-20241218172223836" style="zoom: 33%;" />

Large Language Models (LLMs) have shown exciting performance in listwise passage ranking. Due to the limited input length, existing methods often adopt the *sliding window* strategy. Such a strategy, though effective, is inefficient as it involves repetitive and serialized processing, which usually re-evaluates relevant passages multiple times. As a result, it incurs redundant API costs, which are proportional to the number of inference tokens. The development of long-context LLMs enables the *full ranking* of all passages within a single inference, avoiding redundant API costs. In this paper, we conduct a comprehensive study of long-context LLMs for ranking tasks in terms of efficiency and effectiveness. Surprisingly, our experiments reveal that full ranking with long-context LLMs can deliver superior performance in the supervised fine-tuning setting with a huge efficiency improvement. Furthermore, we identify two limitations of fine-tuning the full ranking model based on existing methods: (1) sliding window strategy fails to produce a full ranking list as a training label, and (2) the language modeling loss cannot emphasize top-ranked passage IDs in the label. To alleviate these issues, we propose a complete listwise label construction approach and a novel importance-aware learning objective for full ranking. Experiments show the superior performance of our method over baselines.

## üì¶ Environment

#### Step1: Create Conda Environment

```
conda create -n fullrank python=3.9
conda activate fullrank
```

#### Step2: Install jdk

In our project, we utilize JDK version 11.0.8 (other versions may also be compatible).

#### Step3: Install packages

```shell
bash env.sh
```

## üìù How to reproduce the experimental results?

### 1. Effectiveness

For the evaluation of effectiveness, please run the following script:

```shell
bash run_rank_llm.sh
```

The evaluation script use vllm for accelerating. Please place the open-source long-text LLM to be evaluated in `llm/`, and our $\text{RankMistral}_{100}$ in `trained_models/`.

### 2. Efficiency

For the evaluation of efficiency, please run the following script:

```shell
bash test_latency.sh
```

Note that we choose not to use vllm technique for a fair comparison. Below shows the the latency across different LLMs:

<img src="https://8421bcd.oss-cn-beijing.aliyuncs.com/img/image-20241218232539325.png" alt="image-20241218232539325" style="zoom: 50%;" />

## üöÄ How to fine-tune a full ranking model?

The training data is constructed by multi-pass sliding window and the model is optimized with importance-aware loss. Here is the overall framework:

<img src="https://8421bcd.oss-cn-beijing.aliyuncs.com/img/image-20241218200920116.png" alt="image-20241218200920116" style="zoom: 45%;" />

The training data is placed in `training_data/`, which can also be downloaded from [here](https://huggingface.co/datasets/liuwenhan/msmarco_full_ranking_list). The data is generated by performing multi-pass sliding windows based on GPT-4o-2024-08-06.

Run the following code to fine-tune a full ranking model:

```shell
cd training
bash run_train.sh
```

For training with standard language modeling loss, set the parameter `weighted_loss=False`.

## üìû Contact

If you have any question or suggestion related to this project, feel free to open an issue or pull request. You also can email Wenhan Liu ([lwh@ruc.edu.cn](mailto:lwh@ruc.edu.cn)).

## ‚ú® Citation

If you find this repository useful, please consider giving a star ‚≠ê and citation

```

```

We also acknowledge the opens-source repo [RankLLM](https://github.com/castorini/rank_llm/), which is instrumental for this work.

 
